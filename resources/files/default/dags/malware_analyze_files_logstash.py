from __future__ import annotations

import pendulum
import os
import uuid
import boto3
import logging
import subprocess
import time
import yaml
import random
from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

# --- Configuration ---
S3_BUCKET_NAME = "malware"
BASE_PREFIX = "mdata/analyzed/"
ENDPOINT_URL = "http://s3.service:9000"
LOCAL_TMP_DIR = "/usr/share/malware/"
DEFAULT_CLEANUP_DELAY = 15
LOGSTASH_CONFIG_PATH = "/etc/airflow/logstash_hosts.yml"

# --- SSH / SCP Configuration ---
SSH_USER = "root"
SSH_KEY_PATH = "/root/.ssh/rsa"
SSH_OPTS = [
    "-o", "StrictHostKeyChecking=no",
    "-o", "UserKnownHostsFile=/dev/null",
    "-o", "ConnectTimeout=10"
]

# --- Logger ---
log = logging.getLogger(__name__)

# --- MinIO Client ---
def get_minio_client():
    """Retrieves and configures the MinIO S3 client."""
    hook = S3Hook(aws_conn_id="minio_conn")
    creds = hook.get_credentials()
    return boto3.client(
        "s3",
        endpoint_url=ENDPOINT_URL,
        aws_access_key_id=creds.access_key,
        aws_secret_access_key=creds.secret_key,
    )

# --- Read Logstash host YAML ---
def load_logstash_hosts(config_path: str = LOGSTASH_CONFIG_PATH) -> list[str]:
    """Read the Logstash host list from a YAML file.”"""
    try:
        with open(config_path, "r") as f:
            data = yaml.safe_load(f)
            hosts = data.get("logstash_hosts", [])
            if not hosts:
                log.warning("There are no hosts defined in logstash_hosts.yml.")
            else:
                log.info(f"Logstash hosts loaded: {hosts}")
            return hosts
    except FileNotFoundError:
        log.error(f"File not found {config_path}")
        return []
    except Exception as e:
        log.error(f"Error reading {config_path}: {e}")
        return []

# --- Download from S3 ---
def download_files_from_minio(**context):
    conf = context.get("dag_run").conf or {}
    date_path = conf.get("date_path")

    if not date_path:
        log.warning("No ‘date_path’ received. Aborting next DAG.")
        return []

    prefix = f"{BASE_PREFIX}{date_path}"
    os.makedirs(LOCAL_TMP_DIR, exist_ok=True)
    log.info(f"Downloading files from {prefix} to {LOCAL_TMP_DIR}")

    s3_client = get_minio_client()
    response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=prefix)

    if "Contents" not in response or not response["Contents"]:
        log.info(f"There are no files in {prefix}")
        return []

    downloaded_files = []

    for obj in response["Contents"]:
        key = obj["Key"]
        if key.endswith("/"):
            continue

        random_uuid = str(uuid.uuid4())
        dest_path = os.path.join(LOCAL_TMP_DIR, random_uuid)
        lock_path = dest_path + ".lock"

        try:
            log.info(f"Downloading {key} → {dest_path}")
            s3_client.download_file(S3_BUCKET_NAME, key, dest_path)

            with open(lock_path, "w") as lock_file:
                lock_file.write(dest_path + "\n")

            os.chmod(dest_path, 0o664)
            os.chmod(lock_path, 0o664)

            downloaded_files.extend([dest_path, lock_path])
            log.info(f"Ready for Logstash: {dest_path} y {lock_path}")

        except Exception as e:
            log.error(f"Error while downloading/preparing {key}: {e}", exc_info=True)

    context["ti"].xcom_push(key="downloaded_files", value=downloaded_files)
    return downloaded_files

# --- Copy to a Logstash node using SCP ---
def copy_files_to_logstash_node(**context):
    ti = context["ti"]
    downloaded_files = ti.xcom_pull(task_ids="download_files", key="downloaded_files")

    if not downloaded_files:
        log.warning("There are no downloaded files, nothing will be copied.")
        return None

    logstash_hosts = load_logstash_hosts()
    if not logstash_hosts:
        log.error("No Logstash hosts were defined.")
        return None

    # Balance hosts
    random.shuffle(logstash_hosts)

    selected_host = None
    for host in logstash_hosts:
        log.info(f"Copy to Logstash node: {host}")
        success_for_host = True

        for f in downloaded_files:
            success = False
            for attempt in range(1, 4):
                try:
                    dest = f"{SSH_USER}@{host}:{LOCAL_TMP_DIR}"
                    log.info(f"Attempt {attempt}/3: copying {f} to {dest}")

                    subprocess.check_call([
                        "sudo", "scp",
                        "-i", SSH_KEY_PATH,
                        *SSH_OPTS,
                        f,
                        dest
                    ])

                    log.info(f"Successful copy of {f} to {host} (attempt {attempt})")
                    success = True
                    break

                except subprocess.CalledProcessError as e:
                    log.warning(f"Error in attempt to copy {f} to {host}: {e}")
                    if attempt < 3:
                        wait_time = attempt * 2
                        log.info(f"Retrying in {wait_time} seconds...")
                        time.sleep(wait_time)
                except FileNotFoundError:
                    log.error("The command ‘scp’ was not found on the system.")
                    return None

            if not success:
                log.error(f"Could not copy {f} to {host}.")
                success_for_host = False
                break  # if one file fails, we try another host

        if success_for_host:
            selected_host = host
            break  # we're out: everything has been copied correctly to a host

    if selected_host:
        log.info(f"Copy successfully completed to host {selected_host}")
    else:
        log.error("No Logstash host accepted the copy.")

    context["ti"].xcom_push(key="logstash_host", value=selected_host)
    return selected_host

# --- Clean local files ---
def cleanup_local_files(**context):
    ti = context["ti"]
    downloaded_files = ti.xcom_pull(task_ids="download_files", key="downloaded_files") or []

    if not downloaded_files:
        log.info("There are no local files to clean up.")
        return

    log.info(f"Waiting {DEFAULT_CLEANUP_DELAY} seconds before cleaning...")
    time.sleep(DEFAULT_CLEANUP_DELAY)

    for f in downloaded_files:
        try:
            if os.path.exists(f):
                os.remove(f)
                log.info(f"Removed: {f}")
        except Exception as e:
            log.error(f"Error deleting {f}: {e}")

# --- DAG Definition ---
with DAG(
    dag_id="malware_analyze_files_logstash",
    start_date=pendulum.datetime(2025, 9, 3, tz="UTC"),
    schedule=None,
    catchup=False,
    tags=["malware", "logstash", "s3", "analysis"],
) as dag:

    download_files_task = PythonOperator(
        task_id="download_files",
        python_callable=download_files_from_minio,
    )

    copy_files_task = PythonOperator(
        task_id="copy_to_logstash_node",
        python_callable=copy_files_to_logstash_node,
    )

    cleanup_task = PythonOperator(
        task_id="cleanup_local_files",
        python_callable=cleanup_local_files,
    )

    download_files_task >> copy_files_task >> cleanup_task
